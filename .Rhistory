}
jointDf <- as.data.frame(GDP_GDI)
names(jointDf) <- c("GDP", "GDI")
ggplot(jointDf) +
aes(GDI, GDP) +
geom_point()
with(jointDf, sd(GDI))
with(jointDf, sd(GDP))
7/3
# 2.3
# 2.3
head(dataset)
View(dataset)
library(dplyr)
library(tidyr)
library(tidyverse)
rm(list = ls())
options(scipen=999)
library(dplyr)
FRED <- "https://fred.stlouisfed.org/graph/fredgraph.csv"
dataset <- readr::read_csv(paste0(FRED, "?id=A261RL1Q225SBEA,A191RL1Q225SBEA")) %>%
rename(quarter_startdate = DATE, GDI = A261RL1Q225SBEA, GDP = A191RL1Q225SBEA) %>%
arrange(desc(quarter_startdate))
head(dataset)
# 2.2
# Draw 100, 000 times from the univariate normal prior for μ
# that you specified in the previous subproblem
n <- 100000
m <- 2.5
s <- .7
ind <- 1:n
# Prior Distribution
GDP <- tibble(mu = rnorm(n, m, s))
ggplot(GDP) +
geom_density(aes(x = mu ))
rbinorm <- function(n, mu_X, sigma_X, mu_Y, sigma_Y, rho) {
x <- rnorm(n, mean = mu_X, sd = sigma_X)
y <- rnorm(n, mean = mu_Y + rho * sigma_Y / sigma_X * (x - mu_X),
sd = sigma_Y * sqrt((1 + rho) * (1 - rho)))
return(cbind(x, y))
}
sigma_X <- 7/3; sigma_Y <- 7/3; rho <- -1/10
GDP_GDI <- matrix(NA, nrow = n, ncol = 2)
for(i in 1:n){
GDP_GDI[i, ] <- rbinorm(1,GDP$mu[i], sigma_X, GDP$mu[i],sigma_Y, rho)
}
jointDf <- as.data.frame(GDP_GDI)
names(jointDf) <- c("GDP", "GDI")
ggplot(jointDf) +
aes(GDI, GDP) +
geom_point()
with(jointDf, sd(GDI))
with(jointDf, sd(GDP))
7/3
# 2.3
head(dataset)
View(dataset)
m
library(dplyr)
library(tidyr)
library(tidyverse)
rm(list = ls())
options(scipen=999)
library(dplyr)
FRED <- "https://fred.stlouisfed.org/graph/fredgraph.csv"
dataset <- readr::read_csv(paste0(FRED, "?id=A261RL1Q225SBEA,A191RL1Q225SBEA")) %>%
rename(quarter_startdate = DATE, GDI = A261RL1Q225SBEA, GDP = A191RL1Q225SBEA) %>%
arrange(desc(quarter_startdate))
head(dataset)
# 2.2
# Draw 100, 000 times from the univariate normal prior for μ
# that you specified in the previous subproblem
n <- 100000
m <- 2.5
s <- .7
# Prior Distribution
GDP <- tibble(mu = rnorm(n, m, s))
ggplot(GDP) +
geom_density(aes(x = mu ))
# Prior Distribution
GDP <- tibble(mu = rnorm(n, m, s))
ggplot(GDP) +
geom_density(aes(x = mu ))
rbinorm <- function(n, mu_X, sigma_X, mu_Y, sigma_Y, rho) {
x <- rnorm(n, mean = mu_X, sd = sigma_X)
y <- rnorm(n, mean = mu_Y + rho * sigma_Y / sigma_X * (x - mu_X),
sd = sigma_Y * sqrt((1 + rho) * (1 - rho)))
return(cbind(x, y))
}
sigma_X <- 7/3; sigma_Y <- 7/3; rho <- -1/10
GDP_GDI <- matrix(NA, nrow = n, ncol = 2)
for(i in 1:n){
GDP_GDI[i, ] <- rbinorm(1,GDP$mu[i], sigma_X, GDP$mu[i],sigma_Y, rho)
}
jointDf <- as.data.frame(GDP_GDI)
names(jointDf) <- c("GDP", "GDI")
ggplot(jointDf) +
aes(GDI, GDP) +
geom_point()
with(jointDf, sd(GDI))
with(jointDf, sd(GDP))
7/3
# 2.3
head(dataset)
m
# 3
install.packages('tidyquant')
install.packages("tidyquant")
install.packages("tidyquant")
library(tidyquant)
R_f <- tq_get("SGOV", from = "2020-06-01", to = "2022-04-01") %>%
filter(weekdays(date) == "Wednesday") %>%
transmute(R_f = (adjusted - lag(adjusted)) / lag(adjusted)) %>%
na.omit %>%
pull
library(dplyr)
library(tidyr)
library(tidyverse)
R_f <- tq_get("SGOV", from = "2020-06-01", to = "2022-04-01") %>%
filter(weekdays(date) == "Wednesday") %>%
transmute(R_f = (adjusted - lag(adjusted)) / lag(adjusted)) %>%
na.omit %>%
pull
library(tidyquant)
R_f <- tq_get("SGOV", from = "2020-06-01", to = "2022-04-01") %>%
filter(weekdays(date) == "Wednesday") %>%
transmute(R_f = (adjusted - lag(adjusted)) / lag(adjusted)) %>%
na.omit %>%
pull
?tq_get
R_f <- tq_get("SGOV", from = "2020-06-01", to = "2022-04-01") %>%
filter(weekdays(date) == "Wednesday") %>%
transmute(R_f = (adjusted - lag(adjusted)) / lag(adjusted)) %>%
na.omit %>%
pull
library(caret)
library(caret)
library(generics)
library(dplyr)
library(tidyr)
library(tidyverse)
rm(list = ls())
options(scipen=999)
version
version
library(tidyverse)
library(rvest)
library(readr)
library(dplyr)
library(tidyr)
library(tidyverse)
library(janitor)
rm(list = ls())
set.seed(1234)
options(scipen=999)
setwd("/Users/adrianharris/Desktop/Messy-Data-Project")
# Merging the web scrapped datasets of
# the Compare Page of players into one datset
dataList <- vector("list", 25)
for(i in 1:25){
dataList[[i]] <- read.csv(paste0('Data/ComparePage/NewData',i,".csv"))
}
# Now its in long format
df <- do.call(rbind, dataList)
head(df, 4)
# Treating as characters for string regex
df <- df %>%
mutate(Rank = as.character(Rank),
Total.XP = as.character(Total.XP),
Level = as.character(Level))
# Removing the ","
df <- df %>%
mutate(Rank = as.numeric(gsub(",","", Rank)),
Level = as.numeric(gsub(",","", Level)),
Total.XP = as.numeric(gsub(",","", Total.XP)))
str(df)
# Distribution of Levels
# Some Skill go to 120
# Most go to 99
df %>%
filter(Level < 120) %>%
ggplot(.) +
aes(Level) +
geom_histogram()
# Distribution of Levels by Skill
df %>%
filter(Level < 120) %>%
ggplot(.) +
aes(Level) +
geom_histogram() +
facet_wrap(~Skill)
df %>%
filter(Level > 120) %>%
ggplot(.) +
aes(Level) +
geom_histogram() +
facet_wrap(~Skill)
# Removing Rank because it is too telling of your total xp
dfwide <- df %>%
select(-Rank)
# Moving from long to wide so each row is one player in the dataset
dfwide<- reshape(dfwide , idvar = "Player", timevar = "Skill", direction = "wide")
# cleaning the column names
dfwide <- clean_names(dfwide)
head(dfwide)
str(dfwide)
# Dropping All Xp expect the overall  from the dataset
dflevel <- dfwide %>% select(-contains("xp"))
playerOvr <- dfwide %>% select(player, total_xp_overall)
new <- dflevel %>% left_join(playerOvr)
# Now we are left with all the players
# levels with there total xp of the given player
head(new)
# Liner Regression Data Check
mod <- lm(total_xp_overall ~. - player -level_overall, data = new)
summary(mod)
# Featuring Engineering
# EFA took too long there isn't variance to seperate
df.clust <-  new
# Clustering
# Treating NAs with zero
# Clustering was giving number with dealing with NA
df.clust[is.na(df.clust)] <- 0
# Max distance between points to make clusters
dis = dist(df.clust, method = "maximum")
# Making the denogram
hc1 = hclust(dis, method = "complete")
# Plotting
plot(hc1, cex = 0.6 , hang = -1, labels = df.clust$player ,
xlab = "Den")
# Not really that good but well cut the tree at 4 subgroups
sub_grp = cutree(hc1, k = 4)
clustersss = df.clust %>% mutate(cluster = sub_grp)
clustersss <- clustersss  %>% select(player, cluster)
new <- new %>% left_join(clustersss, by = 'player' )
# Type of player: Pure, Iron man and etc
# We are generalizing these skills of the players
# into these types of pures.
# They may not accuattly be X type of pure
# Usaully with pures defence is a hard cut off so that means
# if they have 1 def, 25, and  etc and have like 99 atk and etc
# they are usaully a pure
# Skills below 15 don't show up no the leader baords
# so we will assume they are a level one
# the xp differnece between 1 and 14 isnt that much
# our pures maybe not be true pure but they will be close enough
# to the truth
new[is.na(new)] <-1 # Do all the columns besides Total xp
### Combat pures
# Basic Member's Pure: 60 Attack, 80+ Strength, 82+ Magic, 80+ Range, 1 Defense
new.copy <- new
names(new)
new  <- new %>%
mutate(Basic_member_pure = ifelse( level_atk == 60 &
level_str >=80 &
level_magic >=82 &
level_range >= 80 &
level_def == 1, 1, 0))
# Obby Mauler Pure [EOC]: 1 attack, 60+ Strength, 1 Defense.
new  <- new %>%
mutate(Obby_Mauler_Pure = ifelse( level_atk == 1 &
level_str >=60 &
level_def == 1, 1, 0))
# Attack Pure [EOC]: 60+ Attack, 1 Strength, 1 Defence
new  <- new %>%
mutate(Obby_Mauler_Pure = ifelse( level_atk >= 60 &
level_str == 1 &
level_def == 1, 1, 0))
# Black Pure: 25 Defence, 60+ Attack, 80+ Strength,
# 75+ Constitution, 80+ Range, 82+ Mage
new  <- new %>%
mutate(Black_Pure = ifelse( level_atk >= 60 &
level_str >=80 &
level_magic >=82 &
level_range >= 80 &
level_def == 25 &
level_constiution >= 75, 1, 0))
#Turmoil/Proselyte Pure: 60+ Attack, 99 Strength, 94 Magic,
#80+ Range, 28-35 Defence, 95 Prayer.
new  <- new %>%
mutate(Turmoil_Proselyte_Pure = ifelse( level_atk >= 60 &
level_str == 99 &
level_magic == 94 &
level_range >= 80 &
level_def %in% c(25:35) &
level_prayer == 95, 1, 0))
# Barrows Pure: 70+ attack, 70+ strength,
# 70 defence, 94 magic, 70+ Prayer.
new  <- new %>%
mutate(Barrows_Pure = ifelse(level_atk >= 70  &
level_str >= 70 &
level_magic == 94 &
level_def ==  70 &
level_prayer >=70, 1, 0))
# Anti Pure [EOC]: 99 Attack, 1 Strength,
#99 Defence, 1/52/99 Prayer, 1/50 Magic,
#1/50 Range, 99 Constitution, 1/88/99 Summoning
names(new)
new  <- new %>%
mutate(Anti_Pure = ifelse(level_atk == 99 &
level_str == 1 &
level_magic %in% c(1,50) &
level_def ==  99 &
level_prayer %in% c(1,52,99) &
level_range %in% c(1,50) &
level_summoning %in% c(1,88,99), 1, 0))
# Summoning Tank/Defence Pure [EOC]: 99 Defence,
# 1 Strength, 1 Attack, 1 Magic, 1 Range, 1/43/75/99 Prayer,
# 99 Constitution, 1/99 Summoning.
new  <- new %>%
mutate(Summoning_Tank_Defence = ifelse(level_atk == 1 &
level_str == 1 &
level_magic ==1 &
level_def ==  99 &
level_prayer %in% c(1,43,75,99) &
level_range == 1 &
level_summoning %in% c(1,99) &
level_constiution == 99, 1, 0))
# Skilling Pure
#Level 3 This is the generic skiller;
#they have 10 Constitution and all combat skills are level 1.
#These are the most commonly found type of skiller.
new <- new %>%
mutate(Skiller_Pure = ifelse(level_crafting >= 60 & level_constiution ==1, 1,0))
# Number of 99s
new$number_of_99s <- rowSums(new == 99)
# Number of 120s
new$number_of_120s <- rowSums(new == 120)
# Based on these two plots we will separate the dataset into two sections
# People that have 5 or less 99s and people that have above 5 99s
ggplot(new) +
aes(number_of_99s) +
geom_histogram()
ggplot(new) +
aes(number_of_120s) +
geom_histogram()
# Dropping them because when modeling 99s and 120s are too predictive of total xp
# which makes sense More 99s more xp high total over xp
above5 <- new %>%
filter(number_of_99s > 5) %>%
select(-number_of_99s, -number_of_120s)
below5 <- new %>%
filter(number_of_99s <= 5)%>%
select(-number_of_99s, -number_of_120s)
write.csv(above5,"Data/feature_eng_above_5_99s_data.csv", row.names = FALSE)
write.csv(below5,"Data/feature_eng_below_5_99s_data.csv", row.names = FALSE)
# Now its in long format
length(unique(df$Player))
View(dataList[[1]])
View(dataList[[25]])
View(dataList[[20]])
# load packages
library(tidyverse)
library(readr)
library(dplyr)
library(tidyr)
library(tidymodels)
library(ROCR)
rm(list = ls())
options(scipen=999)
setwd("~/Desktop/hw6_adrian_harris")
df <- read.csv('Data/DOHMH_New_York_City_Restaurant_Inspection_Results.csv')
dim(df)
library(tidyverse)
library(rvest)
library(readr)
library(dplyr)
library(tidyr)
library(tidyverse)
rm(list = ls())
set.seed(1234)
options(scipen=999)
setwd("/Users/adrianharris/Desktop/MDML-Project")
# Attack
urls <- c()
library(tidyverse)
library(rvest)
library(readr)
library(dplyr)
library(tidyr)
library(tidyverse)
rm(list = ls())
set.seed(1234)
options(scipen=999)
setwd("/Users/adrianharris/Desktop/MDML-Project")
# Attack
urls <- c()
pagesRange <- 1:57797
maxPage <- 264
for(i in 1:57797){
urls[i] <- paste0("https://secure.runescape.com/m=hiscore/ranking?category_type=0&table=1&date=1648423839978&time_filter=0&page=",i)
}
spots <- round(max(pagesRange)/maxPage)
n <- length(pagesRange)
idx <- sample(rep(1:spots, each = ceiling(n /spots))[1:n], replace = F)
urlList <- split(urls, idx)
library(tidyverse)
library(rvest)
library(readr)
library(dplyr)
library(tidyr)
library(tidyverse)
rm(list = ls())
set.seed(1234)
options(scipen=999)
setwd("/Users/adrianharris/Desktop/MDML-Project")
# Attack
urls <- c()
pagesRange <- 1:57797
maxPage <- 264
for(i in 1:57797){
urls[i] <- paste0("https://secure.runescape.com/m=hiscore/ranking?category_type=0&table=1&date=1648423839978&time_filter=0&page=",i)
}
spots <- round(max(pagesRange)/maxPage)
n <- length(pagesRange)
idx <- sample(rep(1:spots, each = ceiling(n /spots))[1:n], replace = F)
urlList <- split(urls, idx)
dataGrabber <- function(url){
rank <- read_html(url) %>%
html_nodes(xpath = "//td[@class='col1 align']") %>%
html_text(trim=TRUE)
name <- read_html(url) %>%
html_nodes(xpath = "//td[@class='col2']") %>%
html_text(trim=TRUE)
level <- read_html(url) %>%
html_nodes(xpath = "//td[@class='col3 align']") %>%
html_text(trim=TRUE)
xp <- read_html(url) %>%
html_nodes(xpath = "//td[@class='col4 align']") %>%
html_text(trim=TRUE)
xp <- as.numeric(gsub(",","",xp))
df <- tibble('Player' = name,
'Level' = level,
'Xp' = xp)
return(df)
}
# Randomly select pages from the full length of pages
# then sample with out replacement
# keeo that vector of values so you don't use them agian
# keeping doing that every day till the dataset is built
dataList <- vector("list",length(1:maxPage))
# Page
for(i in 1:length(dataList)){
dataList[[i]] <- dataGrabber(urlList[[11]][i])
}
df <- do.call(rbind, dataList)
nrow(df)
head(df)
View(df)
write.csv(df,"Data/Atk/RunescapeAtkLeaderboard11.csv", row.names = FALSE)
setwd("/Users/adrianharris/Desktop/Messy-Data-Project")
write.csv(df,"Data/Atk/RunescapeAtkLeaderboard11.csv", row.names = FALSE)
library(tidyverse)
library(rvest)
library(readr)
library(dplyr)
library(tidyr)
library(tidyverse)
rm(list = ls())
set.seed(1234)
options(scipen=999)
setwd("/Users/adrianharris/Desktop/Messy-Data-Project")
# Attack
urls <- c()
pagesRange <- 1:57797
maxPage <- 264
# Making urls
for(i in 1:57797){
urls[i] <- paste0("https://secure.runescape.com/m=hiscore/ranking?category_type=0&table=1&date=1648423839978&time_filter=0&page=",i)
}
# These lines of code takes the urls
# Breaks them into 219 chunks of around 220-250 urls within each chunk
# In theory there are randomly selected player from arcoss all atk skill levels (99-15)
# Runescape doesn't report below 15
spots <- round(max(pagesRange)/maxPage)
n <- length(pagesRange)
idx <- sample(rep(1:spots, each = ceiling(n /spots))[1:n], replace = F)
urlList <- split(urls, idx)
dataGrabber <- function(url){
rank <- read_html(url) %>%
html_nodes(xpath = "//td[@class='col1 align']") %>%
html_text(trim=TRUE)
name <- read_html(url) %>%
html_nodes(xpath = "//td[@class='col2']") %>%
html_text(trim=TRUE)
level <- read_html(url) %>%
html_nodes(xpath = "//td[@class='col3 align']") %>%
html_text(trim=TRUE)
xp <- read_html(url) %>%
html_nodes(xpath = "//td[@class='col4 align']") %>%
html_text(trim=TRUE)
xp <- as.numeric(gsub(",","",xp))
df <- tibble('Player' = name,
'Level' = level,
'Xp' = xp)
return(df)
}
# Randomly select pages from the full length of pages
# then sample with out replacement
# keeo that vector of values so you don't use them agian
# keeping doing that every day till the dataset is built
dataList <- vector("list",length(1:maxPage))
# Page
for(i in 1:length(dataList)){
dataList[[i]] <- dataGrabber(urlList[[12]][i])
}
i
df <- do.call(rbind, dataList)
# Remember Change the Page number
write.csv(df,"Data/Atk/RunescapeAtkLeaderboard12.csv", row.names = FALSE)
head(df)
View(df)
